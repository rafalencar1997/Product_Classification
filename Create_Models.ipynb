{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "\n",
    "from keras import losses, models, optimizers, initializers, regularizers\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import (Layer, Input, Flatten, Dropout, BatchNormalization, Reshape, Embedding,\n",
    "                          MaxPool1D, AveragePooling1D, GlobalAveragePooling1D,\n",
    "                          Conv1D, SeparableConv1D, Dense, LeakyReLU, ReLU, Activation,\n",
    "                          LSTM, SimpleRNNCell, Bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 lines skipped (not labeled)\n",
      "0 lines skipped (not text)\n"
     ]
    }
   ],
   "source": [
    "filename = 'datasets/amazon_co-ecommerce_sample.csv'\n",
    "text_field='name'\n",
    "\n",
    "dataset = Dataset(filename)\n",
    "dataset.load(text_field=text_field, label_field='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 13363\n"
     ]
    }
   ],
   "source": [
    "tokenizer = dataset.tokenize()\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = 100\n",
    "max_vocab_size = 20000\n",
    "\n",
    "sentences = dataset.text.transform(lambda t: t.split())\n",
    "\n",
    "embedding = Word2Vec(sentences, size=size,\n",
    "                     max_vocab_size=max_vocab_size)\n",
    "\n",
    "output_file = 'embeddings/{}_s{}.txt'.format('word2vec', size)\n",
    "embedding.wv.save_word2vec_format(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_filename = 'embeddings/word2vec_s100.txt'\n",
    "\n",
    "embedding = KeyedVectors.load_word2vec_format(embedding_filename)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
    "                            output_dim=embedding.vector_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=dataset.max_text(),\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SepCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sepcnn = False\n",
    "if use_sepcnn: \n",
    "    \n",
    "    # Hyper Parameters\n",
    "    blocks       = 4\n",
    "    filters      = 32\n",
    "    kernel_size  = 5\n",
    "    dropout_rate = 0.2\n",
    "    pool_size    = 3\n",
    "    \n",
    "    input_layer = Input(shape=(dataset.max_text(),), dtype='int32')\n",
    "    x = embedding_layer(input_layer)\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        x = Dropout(rate=dropout_rate)(x)   \n",
    "        x = SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "                            bias_initializer='random_uniform', depthwise_initializer='random_uniform',\n",
    "                            padding='same')(x)\n",
    "        x = SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
    "                            bias_initializer='random_uniform', depthwise_initializer='random_uniform',\n",
    "                            padding='same')(x) \n",
    "        x = MaxPool1D(pool_size=pool_size)(x) \n",
    "\n",
    "    x = SeparableConv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu',\n",
    "                        bias_initializer='random_uniform', depthwise_initializer='random_uniform',\n",
    "                        padding='same')(x) \n",
    "    x = SeparableConv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu',\n",
    "                        bias_initializer='random_uniform', depthwise_initializer='random_uniform',\n",
    "                        padding='same')(x) \n",
    "    x = GlobalAveragePooling1D()(x) \n",
    "    x = Dropout(rate=dropout_rate)(x) \n",
    "\n",
    "    output_layer = Dense(dataset.label_unique.size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.name = 'sepcnn_ ' + text_field\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jatana LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was based on the models used in this [article](https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f). Here you can find the [GitHub Repository](https://github.com/jatana-research/Text-Classification/blob/master/RNN.ipynb) for this model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 104)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 104, 100)          1336400   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               51456     \n",
      "=================================================================\n",
      "Total params: 1,548,656\n",
      "Trainable params: 1,548,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_jatana_lstm = True\n",
    "if use_jatana_lstm:\n",
    "    \n",
    "    # Hyper Parameters\n",
    "    units       = 100\n",
    "    \n",
    "    input_layer = Input(shape=(dataset.max_text(),), dtype='int32')\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = Bidirectional(LSTM(units))(x)\n",
    "    output_layer = Dense(dataset.label_unique.size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.name = 'lstm_jatana_' + text_field\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('created_models/'+ model.name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
