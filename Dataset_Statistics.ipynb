{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import stats\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'amazon'\n",
    "\n",
    "PATH_DATASET = '/Users/rafalencar/Documents/Datasets/Products_Catalog/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ab1cfdf38a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amazon_co-ecommerce_sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH_DATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('amazon_co-ecommerce_sample', PATH_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "filename = PATH_DATASET + 'amazon_co-ecommerce_sample.csv'\n",
    "filenamejsonl = PATH_DATASET + 'amazon_co-ecommerce_dataset.jsonl'\n",
    "columns_to_mantain = ['product_name', 'product_description', 'product_information',\n",
    "                      'amazon_category_and_sub_category']\n",
    "\n",
    "columns_new_name   = ['name', 'description', 'information' ,'category']\n",
    "\n",
    "readFile = open(filename, 'r')\n",
    "reader = csv.reader(readFile)\n",
    "       \n",
    "data = list()\n",
    "for row in reader:\n",
    "    data.append(row)\n",
    "            \n",
    "dataset = pd.DataFrame.from_dict(data)\n",
    "dataset.columns = dataset.loc[0]\n",
    "dataset = dataset.drop([0])\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset = dataset[columns_to_mantain]\n",
    "\n",
    "dataset.head(5)\n",
    "\n",
    "dataset.columns = columns_new_name\n",
    "           \n",
    "readFile.close()\n",
    "        \n",
    "dataset.to_json(filenamejsonl, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Labeled Dataset Shape:  690\n",
      "Labeled Dataset Shape:  9310\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_information</th>\n",
       "      <th>product_description</th>\n",
       "      <th>amazon_category_and_sub_category</th>\n",
       "      <th>amazon_category_and_sub_category.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hornby 2014 Catalogue</td>\n",
       "      <td>Technical Details Item Weight640 g Product Dim...</td>\n",
       "      <td>Product Description Hornby 2014 Catalogue Box ...</td>\n",
       "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
       "      <td>Hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FunkyBuys速 Large Christmas Holiday Express Fes...</td>\n",
       "      <td>Technical Details Manufacturer recommended age...</td>\n",
       "      <td>Size Name:Large FunkyBuys速 Large Christmas Hol...</td>\n",
       "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
       "      <td>Hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLASSIC TOY TRAIN SET TRACK CARRIAGES LIGHT EN...</td>\n",
       "      <td>Technical Details Manufacturer recommended age...</td>\n",
       "      <td>BIG CLASSIC TOY TRAIN SET TRACK CARRIAGE LIGHT...</td>\n",
       "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
       "      <td>Hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HORNBY Coach R4410A BR Hawksworth Corridor 3rd</td>\n",
       "      <td>Technical Details Item Weight259 g Product Dim...</td>\n",
       "      <td>Hornby 00 Gauge BR Hawksworth 3rd Class W 2107...</td>\n",
       "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
       "      <td>Hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornby 00 Gauge 0-4-0 Gildenlow Salt Co. Steam...</td>\n",
       "      <td>Technical Details Item Weight159 g Product Dim...</td>\n",
       "      <td>Product Description Hornby RailRoad 0-4-0 Gild...</td>\n",
       "      <td>Hobbies &gt; Model Trains &amp; Railway Sets &gt; Rail V...</td>\n",
       "      <td>Hobbies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name  \\\n",
       "0                              Hornby 2014 Catalogue   \n",
       "1  FunkyBuys速 Large Christmas Holiday Express Fes...   \n",
       "2  CLASSIC TOY TRAIN SET TRACK CARRIAGES LIGHT EN...   \n",
       "3     HORNBY Coach R4410A BR Hawksworth Corridor 3rd   \n",
       "4  Hornby 00 Gauge 0-4-0 Gildenlow Salt Co. Steam...   \n",
       "\n",
       "                                 product_information  \\\n",
       "0  Technical Details Item Weight640 g Product Dim...   \n",
       "1  Technical Details Manufacturer recommended age...   \n",
       "2  Technical Details Manufacturer recommended age...   \n",
       "3  Technical Details Item Weight259 g Product Dim...   \n",
       "4  Technical Details Item Weight159 g Product Dim...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Product Description Hornby 2014 Catalogue Box ...   \n",
       "1  Size Name:Large FunkyBuys速 Large Christmas Hol...   \n",
       "2  BIG CLASSIC TOY TRAIN SET TRACK CARRIAGE LIGHT...   \n",
       "3  Hornby 00 Gauge BR Hawksworth 3rd Class W 2107...   \n",
       "4  Product Description Hornby RailRoad 0-4-0 Gild...   \n",
       "\n",
       "                    amazon_category_and_sub_category  \\\n",
       "0  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
       "1  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
       "2  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
       "3  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
       "4  Hobbies > Model Trains & Railway Sets > Rail V...   \n",
       "\n",
       "  amazon_category_and_sub_category.1  \n",
       "0                            Hobbies  \n",
       "1                            Hobbies  \n",
       "2                            Hobbies  \n",
       "3                            Hobbies  \n",
       "4                            Hobbies  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(PATH_DATASET + 'amazon_co-ecommerce_sample_Dataset.csv')\n",
    "dataset = dataset.replace(np.nan, '', regex=True)\n",
    "\n",
    "dataToLabel = pd.read_csv(PATH_DATASET + 'amazon_co-ecommerce_sample_NoLabel.csv')\n",
    "dataToLabel = dataToLabel.replace(np.nan, '', regex=True)\n",
    "\n",
    "print('Not Labeled Dataset Shape: ', dataToLabel.shape[0])\n",
    "print('Labeled Dataset Shape: ', dataset.shape[0])\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_FL = \"amazon_category_and_sub_category.1\"\n",
    "\n",
    "labels = dataset[CATEGORY_FL].transform(lambda x: x.split(' > ')[0])\n",
    "labels_freq = labels.value_counts() / labels.count()\n",
    "labels_freq.plot(kind='bar', figsize=(15,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY    = \"amazon_category_and_sub_category\"\n",
    "\n",
    "min_samples = 2\n",
    "group_data = dataset[CATEGORY].value_counts()\n",
    "group_data = group_data[group_data >= min_samples]\n",
    "labels_freq = group_data / dataset[CATEGORY].count()\n",
    "\n",
    "print('All labels with minimum %d samples: %d' % (min_samples, labels_freq.count()))\n",
    "labels_freq.plot(kind='barh', figsize=(15, 80), width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 1\n",
    "groupby = dataset.groupby(CATEGORY).product_name.count()\n",
    "filtered = groupby[groupby <= MAX_SAMPLES]\n",
    "\n",
    "#Observa巽達o, filtrar os labels pelo n炭mero de exemplares\n",
    "print(\"All Labels with maximum %d samples: \" % MAX_SAMPLES, filtered.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetFilt = dataset[~dataset[CATEGORY].isin(filtered.index)]\n",
    "labels = datasetFilt[CATEGORY].unique()\n",
    "LABELS = labels.shape[0]\n",
    "print(\"Total Filtered Labels: \", labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_size = []\n",
    "X_used = \"product_description\"\n",
    "for item in dataset[X_used]:\n",
    "    split = item.split()\n",
    "    descr_size.append(len(split))\n",
    "\n",
    "mode = stats.mode(descr_size)\n",
    "mean = np.mean(descr_size)\n",
    "std  = np.std(descr_size)\n",
    "maxL = max(descr_size)\n",
    "minL = min(descr_size)\n",
    "\n",
    "ratio = dataset.shape[0]/mean\n",
    "\n",
    "print('Mode Description Length:', mode.mode[0], 'with', mode.count[0], 'samples') \n",
    "print('Mean Description Length: ', round(mean,2))  \n",
    "print('Std Description Length: ', round(std,2))\n",
    "print('Max Description Length: ', maxL)  \n",
    "print('Min Description Length: ', minL) \n",
    "print('S/W Description Ratio: ', int(ratio)) \n",
    "\n",
    "plt.figure(figsize=(25,4))\n",
    "plt.hist(descr_size, bins=maxL-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_size = []\n",
    "X_used = \"product_information\"\n",
    "for item in dataset[X_used]:\n",
    "    split = item.split()\n",
    "    info_size.append(len(split))\n",
    "    \n",
    "mode = stats.mode(info_size)\n",
    "mean = np.mean(info_size)\n",
    "std  = np.std(info_size)\n",
    "maxL = max(info_size)\n",
    "minL = min(info_size)\n",
    "\n",
    "ratio = dataset.shape[0]/mean\n",
    "  \n",
    "print('Mode Description Length:', mode.mode[0], 'with', mode.count[0], 'samples')   \n",
    "print('Mean Title Length: ', round(mean,2)) \n",
    "print('Std Title Length: ', round(std,2))  \n",
    "print('Max Title Length: ', maxL)  \n",
    "print('Min Title Length: ', minL)  \n",
    "print('S/W Title Ratio: ', int(ratio))\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.hist(info_size, bins=maxL-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_size = []\n",
    "X_used = \"product_name\"\n",
    "for item in dataset[X_used]:\n",
    "    split = item.split()\n",
    "    title_size.append(len(split))\n",
    "    \n",
    "mode = stats.mode(title_size)\n",
    "mean = np.mean(title_size)\n",
    "std  = np.std(title_size)\n",
    "maxL = max(title_size)\n",
    "minL = min(title_size)\n",
    "\n",
    "ratio = dataset.shape[0]/mean\n",
    "  \n",
    "print('Mode Description Length:', mode.mode[0], 'with', mode.count[0], 'samples')   \n",
    "print('Mean Title Length: ', round(mean,2)) \n",
    "print('Std Title Length: ', round(std,2))  \n",
    "print('Max Title Length: ', maxL)  \n",
    "print('Min Title Length: ', minL)  \n",
    "print('S/W Title Ratio: ', int(ratio))\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.hist(title_size, bins=maxL-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequency_distribution_of_ngrams(sample_texts, ngram_range=(1, 2), num_ngrams=50):\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "plot_frequency_distribution_of_ngrams(dataset[X_used], ngram_range=(1, 2), num_ngrams=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[CATEGORY].unique()\n",
    "LABELS = labels.shape[0]\n",
    "print(\"Total Labels: \", labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_used = 'product_information'\n",
    "dataX = dataset[X_used]\n",
    "\n",
    "if X_used == 'product_name': \n",
    "    MAX_SEQUENCE_LENGTH = 1.1*max(title_size) \n",
    "elif X_used == 'product_description':  \n",
    "    MAX_SEQUENCE_LENGTH = 1.1*max(descr_size)\n",
    "else:\n",
    "    MAX_SEQUENCE_LENGTH = 1.1*max(info_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "PATH_DATA_MODELS   = 'data_models/'\n",
    "\n",
    "FILE_WORD2VEC      = PATH_DATA_MODELS + DATASET + '_' + X_used +'_word2vec_s' + str(EMBEDDING_DIM) + '.model'\n",
    "FILE_TOKENIZER     = PATH_DATA_MODELS + DATASET + '_' + X_used +'_tokenizer.sav'\n",
    "FILE_VECTORIZER    = PATH_DATA_MODELS + DATASET + '_' + X_used +'_vectorizer.sav'\n",
    "FILE_LABEL_ENCODER = PATH_DATA_MODELS + DATASET +'_label_encoder.sav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for item in dataX:\n",
    "    split = item.split()\n",
    "    sentences.append(split)\n",
    "    \n",
    "start = time()\n",
    "word2vec = Word2Vec(sentences, size=EMBEDDING_DIM)#, window=5, min_count=1, workers=4)\n",
    "print(\"Execution Time:\", round(time() - start, 2), 's')\n",
    "print(\"Number of Word Vectors:\", len(list(word2vec.wv.vocab)))\n",
    "\n",
    "# Save Word2Vec\n",
    "word2vec.save(FILE_WORD2VEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(dataX)\n",
    "print(\"Number of Tokens:\",len(tokenizer.word_counts))\n",
    "\n",
    "# Save Tokenizer\n",
    "pickle.dump(tokenizer, open(FILE_TOKENIZER, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "vectorizer  = TfidfVectorizer(max_features=MAX_NB_WORDS, stop_words='english')\n",
    "vectorizer.fit(dataX)\n",
    "\n",
    "# Save Vectorizer\n",
    "pickle.dump(vectorizer, open(FILE_VECTORIZER, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "# Save Label Encoder \n",
    "pickle.dump(le, open(FILE_LABEL_ENCODER, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree('Root')    \n",
    "for label in labels:\n",
    "    split = label.split(' > ')\n",
    "    tree = tree.populateTree(split, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataX\n",
    "y_train = dataset[CATEGORY]\n",
    "tree.sampleTree(x_train, y_train, 0)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
